{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r-kowalczyk/graph-link-prediction/blob/main/notebooks/colab_runner.ipynb)\n",
        "\n",
        "# Graph Link Prediction - Google Colab Runner\n",
        "\n",
        "This notebook runs the project using the same CLI as the local quickstart (`graph-lp`), but with a Colab oriented configuration that expects the full dataset on Google Drive and can use a GPU.\n",
        "\n",
        "## Two ways to run\n",
        "\n",
        "- **Quickstart sanity run (CPU, bundled data)**: uses `configs/quickstart.yaml` and finishes quickly (same as README quickstart).\n",
        "- **Full run (Drive data, optional GPU)**: uses `configs/full.yaml` and trains the full hybrid pipeline.\n",
        "\n",
        "Both modes use the same CLI commands. The only thing that changes is the config file, which controls the data paths, model choice, and output directory.\n",
        "\n",
        "## What you get\n",
        "\n",
        "After training completes, check the output directory for:\n",
        "\n",
        "- `metrics.json`: performance metrics (ROC-AUC, PR-AUC, F1, Precision@k, and more)\n",
        "- `config_used.yaml`: the exact config text used for the run\n",
        "- `curves/roc.png`: ROC curve visualisation\n",
        "- `curves/pr.png`: Precision-Recall curve visualisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "If you want to run the full pipeline on the full dataset, you need the data on Google Drive.\n",
        "\n",
        "1) Download the data folder from [here](https://drive.google.com/drive/folders/1XbGVQiNid29Mxt1tjR7gitfxaVwJvc0t?usp=sharing).\n",
        "\n",
        "2) Save the folder in the root of your Google Drive (see `configs/full.yaml` for the expected paths).\n",
        "\n",
        "3) Run the next cell to mount your Google Drive so the notebook can read the CSV files.\n",
        "\n",
        "If you only want to run the quickstart sanity run, you can skip the data download, because `configs/quickstart.yaml` uses a tiny bundled dataset in the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install PyTorch Geometric Dependencies\n",
        "\n",
        "This cell installs PyTorch Geometric and its required dependencies. These are needed for the Node2Vec structural embedding algorithm, which is used by the full hybrid run.\n",
        "\n",
        "If you only plan to run the quickstart sanity run (semantic-only), you can skip this step.\n",
        "\n",
        "The installation automatically detects your PyTorch and CUDA versions to install compatible wheels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pin PyTorch to 2.8.0 because PyTorch Geometric publishes wheels for this pair,\n",
        "# which prevents slow source builds in Colab.\n",
        "import subprocess\n",
        "\n",
        "def _has_nvidia_runtime() -> bool:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"nvidia-smi\"],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL,\n",
        "            check=True,\n",
        "        )\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# Pin PyTorch to 2.8.0. Use CUDA wheels when available; otherwise use CPU wheels.\n",
        "if _has_nvidia_runtime():\n",
        "    torch_index_url = \"https://download.pytorch.org/whl/cu126\"\n",
        "    pyg_wheel_url = \"https://data.pyg.org/whl/torch-2.8.0%2Bcu126.html\"\n",
        "    print(\"Detected NVIDIA runtime. Installing PyTorch 2.8.0 (CUDA 12.6 wheels).\")\n",
        "else:\n",
        "    torch_index_url = \"https://download.pytorch.org/whl/cpu\"\n",
        "    pyg_wheel_url = \"https://data.pyg.org/whl/torch-2.8.0%2Bcpu.html\"\n",
        "    print(\"No NVIDIA runtime detected. Installing PyTorch 2.8.0 (CPU wheels).\")\n",
        "\n",
        "# Note: build tags (+cu126 / +cpu) are present in wheel filenames, but pip can match them with torch==2.8.0.\n",
        "%pip install -q torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url {torch_index_url}\n",
        "\n",
        "# Install PyTorch Geometric binary wheels that match the pinned torch build (avoids source builds).\n",
        "%pip install -q pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f {pyg_wheel_url}\n",
        "%pip install -q torch-geometric\n",
        "\n",
        "import torch\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda:\", torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install the repository (editable)\n",
        "\n",
        "This cell clones the repository and installs it in editable mode.\n",
        "\n",
        "Using an editable install keeps the notebook consistent with the local README workflow and makes the config files (`configs/*.yaml`) available without downloading them separately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/r-kowalczyk/graph-link-prediction.git\n",
        "%cd graph-link-prediction\n",
        "\n",
        "# Install in editable mode so the notebook and the repository code stay in sync.\n",
        "%pip install -q -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confirm the CLI is available and the repository configs are present.\n",
        "!graph-lp --help\n",
        "!ls -lah configs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Quickstart sanity run (recommended)\n",
        "\n",
        "This cell runs a small start to finish training and evaluation.\n",
        "\n",
        "- **Config**: `configs/quickstart.yaml`\n",
        "- **Data**: bundled CSV files in the repository\n",
        "- **Device**: CPU\n",
        "- **Output**: `artifacts_quickstart/<timestamp>/`\n",
        "\n",
        "To check the runtime and dependencies are working before starting the full run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quickstart sanity run (small CPU run on bundled data)\n",
        "!graph-lp train --config configs/quickstart.yaml --device cpu --seed 42\n",
        "!graph-lp evaluate --config configs/quickstart.yaml --device cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Full run (Drive data, optional GPU)\n",
        "\n",
        "Assuming you are in Google Colab and have credits to spend on hardware acceleration, you can run the full pipeline using the below cell.\n",
        "\n",
        "This cell runs the full hybrid pipeline using the dataset you placed on Google Drive (see Prerequisites for access to the full data).\n",
        "\n",
        "- **Config**: `configs/full.yaml`\n",
        "- **Data**: expected under `/content/drive/MyDrive/graph-link-prediction-files/data/full`\n",
        "- **Variant**: `hybrid`\n",
        "- **Device**: set to `auto` so CUDA is used when available\n",
        "\n",
        "The output directory is set explicitly so artefacts are stored on Drive rather than only in the Colab runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full training run (hybrid embeddings, Drive dataset)\n",
        "full_output_directory = \"/content/drive/MyDrive/graph-link-prediction-files/artifacts_full\"\n",
        "\n",
        "!graph-lp train --config configs/full.yaml --variant hybrid --device auto --seed 42 --output-dir {full_output_directory}\n",
        "!graph-lp evaluate --config configs/full.yaml --device auto --output-dir {full_output_directory}\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
