{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r-kowalczyk/graph-link-prediction/blob/main/notebooks/colab_runner.ipynb)\n",
        "\n",
        "# Graph Link Prediction - Google Colab Runner\n",
        "\n",
        "This notebook runs the project using the same CLI as the local quickstart (`graph-lp`), but with a Colab oriented configuration that expects the full dataset on Google Drive and can use a GPU.\n",
        "\n",
        "## Two ways to run\n",
        "\n",
        "- **Quickstart sanity run (CPU, bundled data)**: uses `configs/quickstart.yaml` and finishes quickly. This matches the idea of the README quickstart.\n",
        "- **Full run (Drive data, optional GPU)**: uses `configs/full.yaml` and trains the full hybrid pipeline.\n",
        "\n",
        "Both modes use the same CLI commands. The only thing that changes is the config file, which controls the data paths, model choice, and output directory.\n",
        "\n",
        "## What you get\n",
        "\n",
        "After training completes, check the output directory for:\n",
        "\n",
        "- `metrics.json`: performance metrics (ROC-AUC, PR-AUC, F1, Precision@k, and more)\n",
        "- `config_used.yaml`: the exact config text used for the run\n",
        "- `curves/roc.png`: ROC curve visualisation\n",
        "- `curves/pr.png`: Precision-Recall curve visualisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "If you want to run the full pipeline on the full dataset, you need the data on Google Drive.\n",
        "\n",
        "1) Download the data folder from [here](https://drive.google.com/drive/folders/1XbGVQiNid29Mxt1tjR7gitfxaVwJvc0t?usp=sharing).\n",
        "\n",
        "2) Save the folder in the root of your Google Drive (see `configs/full.yaml` for the expected paths).\n",
        "\n",
        "3) Run the next cell to mount your Google Drive so the notebook can read the CSV files.\n",
        "\n",
        "If you only want to run the quickstart sanity run, you can skip the data download, because `configs/quickstart.yaml` uses a tiny bundled dataset in the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install PyTorch Geometric Dependencies\n",
        "\n",
        "This cell installs PyTorch Geometric and its required dependencies. These are needed for the Node2Vec structural embedding algorithm, which is used by the full hybrid run.\n",
        "\n",
        "If you only plan to run the quickstart sanity run (semantic-only), you can skip this step.\n",
        "\n",
        "The installation automatically detects your PyTorch and CUDA versions to install compatible wheels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pin PyTorch to 2.4.0+cu121 because PyTorch Geometric publishes wheels for this pair, which prevents slow source builds in Colab.\n",
        "%pip install -q torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "import torch\n",
        "\n",
        "def install_pytorch_geometric():\n",
        "    \"\"\"Install PyTorch Geometric wheels for the pinned torch 2.4.0 CUDA 12.1 stack.\n",
        "\n",
        "    Parameters: None\n",
        "    Uses the existing torch installation to derive the wheel feed.\n",
        "    Expects torch to be preinstalled at 2.4.0 with CUDA 12.1.\n",
        "    Installs binary wheels so the GPU runtime stays ready without long builds.\n",
        "    \"\"\"\n",
        "    torch_version = torch.__version__.split(\"+\")[0]\n",
        "    cuda_version = torch.version.cuda.replace(\".\", \"\") if torch.version.cuda else None\n",
        "    base_url = \"https://data.pyg.org/whl\"\n",
        "\n",
        "    # Use the wheel feed that matches the pinned torch and CUDA versions so pip downloads binaries instead of compiling from source.\n",
        "    if cuda_version:\n",
        "        print(\n",
        "            f\"Installing PyTorch Geometric for PyTorch {torch_version} with CUDA {cuda_version}\"\n",
        "        )\n",
        "        %pip install -q torch-scatter -f {base_url}/torch-{torch_version}+cu{cuda_version}.html\n",
        "        %pip install -q torch-sparse -f {base_url}/torch-{torch_version}+cu{cuda_version}.html\n",
        "        %pip install -q torch-cluster -f {base_url}/torch-{torch_version}+cu{cuda_version}.html\n",
        "        %pip install -q torch-spline-conv -f {base_url}/torch-{torch_version}+cu{cuda_version}.html\n",
        "        %pip install -q pyg-lib -f {base_url}/torch-{torch_version}+cu{cuda_version}.html\n",
        "    else:\n",
        "        print(f\"Installing PyTorch Geometric for PyTorch {torch_version} (CPU only)\")\n",
        "        %pip install -q torch-scatter -f {base_url}/torch-{torch_version}+cpu.html\n",
        "        %pip install -q torch-sparse -f {base_url}/torch-{torch_version}+cpu.html\n",
        "        %pip install -q torch-cluster -f {base_url}/torch-{torch_version}+cpu.html\n",
        "        %pip install -q torch-spline-conv -f {base_url}/torch-{torch_version}+cpu.html\n",
        "        %pip install -q pyg-lib -f {base_url}/torch-{torch_version}+cpu.html\n",
        "\n",
        "    %pip install -q torch-geometric\n",
        "    print(\"PyTorch Geometric installation complete!\")\n",
        "\n",
        "\n",
        "install_pytorch_geometric()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install the repository (editable)\n",
        "\n",
        "This cell clones the repository and installs it in editable mode.\n",
        "\n",
        "Using an editable install keeps the notebook consistent with the local README workflow and makes the config files (`configs/*.yaml`) available without downloading them separately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/r-kowalczyk/graph-link-prediction.git\n",
        "%cd graph-link-prediction\n",
        "\n",
        "# Install in editable mode so the notebook and the repository code stay in sync.\n",
        "%pip install -q -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confirm the CLI is available and the repository configs are present.\n",
        "!graph-lp --help\n",
        "!ls -lah configs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Quickstart sanity run (recommended)\n",
        "\n",
        "This cell runs a small start to finish training and evaluation.\n",
        "\n",
        "- **Config**: `configs/quickstart.yaml`\n",
        "- **Data**: bundled CSV files in the repository\n",
        "- **Device**: CPU\n",
        "- **Output**: `artifacts_quickstart/<timestamp>/`\n",
        "\n",
        "To check the runtime and dependencies are working before starting the full run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quickstart sanity run (small CPU run on bundled data)\n",
        "!graph-lp train --config configs/quickstart.yaml --device cpu --seed 42\n",
        "!graph-lp evaluate --config configs/quickstart.yaml --device cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Full run (Drive data, optional GPU)\n",
        "\n",
        "Assuming you are in Google Colab and have credits to spend on hardware acceleration, you can run the full pipeline using the below cell.\n",
        "\n",
        "This cell runs the full hybrid pipeline using the dataset you placed on Google Drive (see Prerequisites for access to the full data).\n",
        "\n",
        "- **Config**: `configs/full.yaml`\n",
        "- **Data**: expected under `/content/drive/MyDrive/graph-link-prediction-files/data/full`\n",
        "- **Variant**: `hybrid`\n",
        "- **Device**: set to `auto` so CUDA is used when available\n",
        "\n",
        "The output directory is set explicitly so artefacts are stored on Drive rather than only in the Colab runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full training run (hybrid embeddings, Drive dataset)\n",
        "full_output_directory = \"/content/drive/MyDrive/graph-link-prediction-files/artifacts_full\"\n",
        "\n",
        "!graph-lp train --config configs/full.yaml --variant hybrid --device auto --seed 42 --output-dir {full_output_directory}\n",
        "!graph-lp evaluate --config configs/full.yaml --device auto --output-dir {full_output_directory}\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
